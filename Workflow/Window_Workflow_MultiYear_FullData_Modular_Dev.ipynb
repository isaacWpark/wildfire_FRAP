{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsraster.prep as tr\n",
    "import tsraster.random as random\n",
    "import tsraster.model as model\n",
    "import tsraster.calculate as ca\n",
    "import numpy as np\n",
    "\n",
    "import tsraster.model  as md\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from math import ceil\n",
    "from tsraster.prep import set_df_mindex\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct poisson disk mask, which masks out all pixels not selected by raster mask\n",
    "'''\n",
    "Create raster of cells to be selected (populated as ones) in a raster of background value zero\n",
    "\n",
    ":param raster_mask: name of raster mask - provides dimensions for subsample, and also masks unusable areas - \n",
    "        Remaining sample area is assumed to be contiguous\n",
    ":param outFile: path and name of output mask consisting of a rater image with values of 1 for selected pixels, \n",
    "        and 0 for all other pixels\n",
    ":param k: number of attempts to select a point around each reference point before marking it as inactive\n",
    ":param r: minimum distance (in raster cells) between selected points \n",
    ":return:  list which includes an array of all masked & unnmasked cells, and a dictionary of all selected points.\n",
    "            Also saves the a raster consisting of 0s for all non-selected points, and 1s for all selected points\n",
    "            to the outFile location.\n",
    "'''\n",
    "\n",
    "\n",
    "rasterMask = random.Poisson_Subsample(raster_mask = r\"../Data\\Examples\\buffer\\StatePoly_buf.tif\",\n",
    "                                      outFile = r\"../Data\\Examples\\diskTest.tif\",\n",
    "                                      k=50, \n",
    "                                      r=5)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conduct climate feature extraction across all years of interest\n",
    "\n",
    "\n",
    "'''\n",
    "Extracts summary statistics(features) from multiYear datasets \n",
    "Outputs a series of dataFrames covering distinct (annual or multiYear) time periods as CSV files\n",
    "\n",
    ":param startYears: list of years on which to start feature extraction\n",
    ":param featureData_Path: file path to data from which to extract features\n",
    ":param feature_params: summary statistics(features) to extract from data within each window\n",
    ":param invar_Data: year-invariate data to join with extracted feature data on an annual scale\n",
    ":param out_Path: file path to location at which extracted features should be output as a csv\n",
    ":param window_length: length of window within which to extract features\n",
    ":param window_offset: number of years by which features pertaining to each year are offset from that year\n",
    ":param mask:  mask to apply to data prior to feature extraction\n",
    ":return: no return.  instead, feature data relative to each year of interest is saved as a .csv file at the out_Path location\n",
    "          under the filename FD_Window_XXXX.csv \n",
    "'''\n",
    "ca.multiYear_Window_Extraction(list(range(1970, 2016, 5)), \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Climate/BCM HIST Final 1000m_1950_2016/\",\n",
    "                    feature_params = {\"mean\": None,\"maximum\": None}, \n",
    "                    out_Path = 'C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Pre_Masked/',\n",
    "                    mask = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Examples/buffer/StatePoly_buf.tif\" ,\n",
    "                    length = 5,\n",
    "                    offset = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assembly of all explanatory variables (including features extracted from climate data) \n",
    "#into dataFrames corresponding to each period of interest\n",
    "\n",
    "'''merges annually repeating data into feature data, \n",
    "    as well as climate features extracted for each period of interest \n",
    "    and time-invariant data\n",
    "    \n",
    "    Produces dataFrames for each time period of interest consisting of all explanatory variables \n",
    "    that may be incorporated into model\n",
    "        (Consisting of features extracted from climate data in preceding years, \n",
    "        annually repeating data such as estimated housing density,\n",
    "        and time-invariant data such as rate of lightning strikes or local elevation)\n",
    "        \n",
    "        In cases where the periods of interst span more than a single year, \n",
    "        mean values within each period of interest will be calculated from annually repeating data\n",
    "        \n",
    "    :param startYears: list of years on which to start feature extraction\n",
    "    :param feature_path: path to feature data\n",
    "    :param dataDict: dictionary of filepaths to each raster and the corresponding desired data column name\n",
    "    :param other_Data_path: filepath (including filename) of example file for each annually repeating parameter to be added\n",
    "                         - replace the 4-digit year within each filename with XXXX in each filePath (i.e. tr_XXXX.csv rather than tr.1981.csv)\n",
    "    :param dataNameList: list of intended data names for additional data\n",
    "    :param outPath: filepath for folder in which the output will be placed\n",
    "    :param length: length of period\n",
    "    :param feature offset: number of years by which to offset featue data from period of interest (to allow use of climate comnditions in preceding years)\n",
    "    :param feature length: length of period for features - may desired to differ from period length if based on preceding conditions\n",
    "    :return: no objects returned.  Instead, each annual dataFrame will be saved as a .csv file in the outPath folder\n",
    "            with filename CD_XXXX.csv \n",
    "'''\n",
    "    \n",
    "\n",
    "dataDict = {\"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/CampGrounds/Campgrounds.tif\": [\"Campground\", 32767.0, 0.0], \n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Fire Stations/FireStatDist_Meters.tif\": [\"FireStation_Dist\", -1, 0.0],\n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Fire Stations/AirBaseDist_Meters.tif\": [\"Airfield_Dist\", -1, 0.0],\n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Roads/PrimSecRoads_Dist.tif\": [\"Road_Dist\", -1, 0.0],\n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Electrical/transmissionLines_Dist.tif\": [\"Elec_Dist\", -1, 0.0],\n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Elev_30m_Products/Roughness.tif\": [\"Roughness\", -1, 0.0],\n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Elev_30m_Products/Heterogeneity_m.tif\": [\"Topo_Heterogeneity\", -1, 0.0],\n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Incorporated Cities/2018_Download/Census_Designated_Raster.tif\": [\"City_Bounds\", 255, 0.0],\n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/National Parks/nps_boundary_clip_Raster.tif\": [\"NPS_Bounds\", 255, 0.0],\n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Lightning/WIS Data/Light5yrWIS_Clipped.tif\": [\"Lightning\", -9999, \"NoData\"],\n",
    "        \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/elevation srtm/SRTM_GTOPO_u30_mosaic_Clip.tif\": [\"Elev\", -9999, \"NoData\"]}\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "tr.period_Data_Merge(startYears = list(range(1970, 2016, 5)),   \n",
    "                  feature_path = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Pre_Masked/\", \n",
    "                 dataDict = dataDict,\n",
    "                     other_Data_path = [\"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/SERGOM_Housing/Interpolated/bhc_XXXX_linreg.tif\"],\n",
    "                 dataNameList = [\"Housing_Density\"],\n",
    "                 outPath = 'C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Pre_Masked_5Len/',\n",
    "                length = 5,\n",
    "                feature_offset = 0,\n",
    "                feature_length = 5)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Conversion of fire data into annual dataframes\n",
    "\n",
    "#convert annual fire raster data into annual CSV files, does some minor reformatting for downstream processing\n",
    "'''convert annual fire data rasters into dataFrames corresponding to each period of interest, and export as .CSV files\n",
    "    also does some minor reformatting to prevent problems with downstream processing\n",
    "\n",
    "    \n",
    "    :param startYears: list of years on which to start feature extraction\n",
    "    :param length: length of extraction period, beginning with each startYear (set to 1 for annual values)\n",
    "    :param file_Path: path to target data files (fire data)\n",
    "    :param out_Path: filepath for folder in which the output will be placed\n",
    "    :param output_style: determines nature of output \n",
    "            set to Count to output number of fires in each pixel within each period\n",
    "            set to Mean to output mean number of fires/year over the period\n",
    "            set to Binary to return 1 if burned during the period, 0 otherwise\n",
    "\n",
    "    :return: no objects returned.  Instead, dataFrames will be saved at location outPath\n",
    "            using the filname TD_XXXX.csv\n",
    "\n",
    "'''\n",
    "\n",
    "tr.target_Data_to_csv_multiYear(list(range(1970, 2015, 5)),\n",
    "                                length = 5,\n",
    "                                file_Path = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Actual/Fires/Rasters/\",\n",
    "                                out_Path = 'C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Pre_Masked_5Len/',\n",
    "                               output_type = \"Mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask All data files (combined_data and target_data) using output of Poisson Disk Masking (or other desired mask)\n",
    "\n",
    "'''mask multiple years of data, export the resulting files annually and as multiyear csvs\n",
    "\n",
    "    :param startYears: years on which to begin\n",
    "    :param filepath: folder in which files to be masked are located.  Files mult be formatted as folles:\n",
    "        CD_XXXX_YYYY.csv for combined data, and\n",
    "        TD_XXXX_YYYY.csv for fire data where XXXX indicates the year on which a period of interest begins, \n",
    "                and YYYY indicates the last year within that period of interest\n",
    "    :param maskFile: filepath to data file used for masking\n",
    "    :outPath: filepath for folder in which the output will be placed\n",
    "    :param length: the length of the desired period of interest\n",
    "    :return: masked dataframes of combined data and target data\n",
    "'''\n",
    "\n",
    "combined_Data, target_Data = tr.multiYear_Mask(list(range(1975, 2015, 5)),\n",
    "                                               filePath = 'C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Pre_Masked_5Len/', \n",
    "                                               maskFile = r\"../Data\\Examples\\diskTest.tif\", \n",
    "                                               outPath = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Masked_5Len/\",\n",
    "                                              length = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pixel_id.1</th>\n",
       "      <th>time</th>\n",
       "      <th>aet__maximum</th>\n",
       "      <th>aet__mean</th>\n",
       "      <th>cwd__maximum</th>\n",
       "      <th>cwd__mean</th>\n",
       "      <th>Campground</th>\n",
       "      <th>FireStation_Dist</th>\n",
       "      <th>...</th>\n",
       "      <th>Road_Dist</th>\n",
       "      <th>Elec_Dist</th>\n",
       "      <th>Roughness</th>\n",
       "      <th>Topo_Heterogeneity</th>\n",
       "      <th>City_Bounds</th>\n",
       "      <th>NPS_Bounds</th>\n",
       "      <th>Lightning</th>\n",
       "      <th>Elev</th>\n",
       "      <th>Housing_Density</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134588</td>\n",
       "      <td>134588</td>\n",
       "      <td>134588</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>104.054550</td>\n",
       "      <td>38.798958</td>\n",
       "      <td>75.966362</td>\n",
       "      <td>40.992970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.043740</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073291</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134596</td>\n",
       "      <td>134596</td>\n",
       "      <td>134596</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>155.080505</td>\n",
       "      <td>58.283031</td>\n",
       "      <td>42.300999</td>\n",
       "      <td>20.542719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26248.809</td>\n",
       "      <td>...</td>\n",
       "      <td>6708.204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.402580</td>\n",
       "      <td>436.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073291</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>135555</td>\n",
       "      <td>135555</td>\n",
       "      <td>135555</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>104.839996</td>\n",
       "      <td>35.365177</td>\n",
       "      <td>89.362000</td>\n",
       "      <td>42.025677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35383.613</td>\n",
       "      <td>...</td>\n",
       "      <td>12165.525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.124300</td>\n",
       "      <td>506.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114690</td>\n",
       "      <td>636.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136483</td>\n",
       "      <td>136483</td>\n",
       "      <td>136483</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>164.182999</td>\n",
       "      <td>62.888699</td>\n",
       "      <td>34.011501</td>\n",
       "      <td>15.720217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26419.690</td>\n",
       "      <td>...</td>\n",
       "      <td>8944.271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.852970</td>\n",
       "      <td>634.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073291</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136523</td>\n",
       "      <td>136523</td>\n",
       "      <td>136523</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>144.088501</td>\n",
       "      <td>52.848095</td>\n",
       "      <td>54.305500</td>\n",
       "      <td>22.716408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57775.426</td>\n",
       "      <td>...</td>\n",
       "      <td>13000.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.113197</td>\n",
       "      <td>1119.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182597</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_id  Unnamed: 0  pixel_id.1           time  aet__maximum  aet__mean  \\\n",
       "0    134588      134588      134588  197501_197912    104.054550  38.798958   \n",
       "1    134596      134596      134596  197501_197912    155.080505  58.283031   \n",
       "2    135555      135555      135555  197501_197912    104.839996  35.365177   \n",
       "3    136483      136483      136483  197501_197912    164.182999  62.888699   \n",
       "4    136523      136523      136523  197501_197912    144.088501  52.848095   \n",
       "\n",
       "   cwd__maximum  cwd__mean  Campground  FireStation_Dist  ...   Road_Dist  \\\n",
       "0     75.966362  40.992970         0.0         25000.000  ...       0.000   \n",
       "1     42.300999  20.542719         0.0         26248.809  ...    6708.204   \n",
       "2     89.362000  42.025677         0.0         35383.613  ...   12165.525   \n",
       "3     34.011501  15.720217         0.0         26419.690  ...    8944.271   \n",
       "4     54.305500  22.716408         0.0         57775.426  ...   13000.000   \n",
       "\n",
       "   Elec_Dist  Roughness  Topo_Heterogeneity  City_Bounds  NPS_Bounds  \\\n",
       "0        0.0  36.043740               274.0          0.0         0.0   \n",
       "1        0.0  41.402580               436.0          0.0         0.0   \n",
       "2        0.0  45.124300               506.0          0.0         0.0   \n",
       "3        0.0  57.852970               634.0          0.0         0.0   \n",
       "4        0.0  44.113197              1119.0          0.0         0.0   \n",
       "\n",
       "   Lightning    Elev  Housing_Density  year  \n",
       "0   0.073291    72.0             0.75  1975  \n",
       "1   0.073291   307.0             0.00  1975  \n",
       "2   0.114690   636.0             0.00  1975  \n",
       "3   0.073291   318.0             0.00  1975  \n",
       "4   0.182597  1207.0             0.00  1975  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temporary for testing only - read in combined & target data \n",
    "\n",
    "combined_Data = pd.read_csv('C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Masked_5Len/CD_1975_2010_Masked_5Len.csv')\n",
    "target_Data = pd.read_csv('C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Masked_5Len/TD_1975_2010_Masked_5Len.csv')\n",
    "combined_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Python3\\Anaconda3\\envs\\ts-raster\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Python3\\Anaconda3\\envs\\ts-raster\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Python3\\Anaconda3\\envs\\ts-raster\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Python3\\Anaconda3\\envs\\ts-raster\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixels_Years MSE Overall:  0.0031630360036847788\n",
      "pixels_Years R2 Overall:  -235.2166171283783\n",
      "\n",
      "\n",
      "pixels MSE Overall:  0.0046302221730704415\n",
      "pixels R2 Overall:  -227.81413208178648\n",
      "\n",
      "\n",
      "years MSE Overall:  0.003662204597641387\n",
      "years R2 Overall:  -233.76850680149306\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Conduct elastic net regularization\n",
    "\n",
    "'''Conduct elastic net regressions on data, with k-fold cross-validation conducted independently \n",
    "      across both years and pixels. \n",
    "      Returns mean model MSE and R2 when predicting fire risk at \n",
    "      A) locations outside of the training dataset\n",
    "      B) years outside of the training dataset\n",
    "      C) locations and years outside of the training dataset\n",
    "\n",
    "    Returns a list of objects, consisting of:\n",
    "      0: Combined_Data file with testing/training groups labeled\n",
    "      1: Target Data file with testing/training groups labeled\n",
    "      2: summary dataFrame of MSE and R2 for each model run\n",
    "          (against holdout data representing either novel locations, novel years, or both)\n",
    "      3: list of elastic net models for use in predicting Fires in further locations/years\n",
    "      4: list of list of years not used in model training for each run\n",
    "      5: parameters selected during hypertuning, to be used in annual predictions\n",
    "  '''\n",
    "\n",
    "  #param combined_Data: explanatory factors to be used in predicting fire risk\n",
    "  #param target_Data: observed fire occurrences\n",
    "  #param varsToGroupBy: list of (2) column names from combined_Data & target_Data to be used in creating randomized groups\n",
    "  #param groupVars: list of (2) desired column names for the resulting randomized groups\n",
    "  #param testGroups: number of distinct groups into which data sets should be divided (for each of two variables) \n",
    "  #param DataFields: column names of data to be used in modelling\n",
    "  #param outPath:  Location in which to export pickle files and summary statistics\n",
    "  #param params: list of parameters to be used in hypertuning\n",
    "  \n",
    "\n",
    "\n",
    "CrossVal_Output = model.elasticNet_2dimTest(combined_Data, target_Data, [\"pixel_id\", \"year\"], [\"pixel_group\", \"year_group\"], \n",
    "                                            testGroups = [10, 3], \n",
    "                                            DataFields = ['aet__mean', 'cwd__maximum', 'cwd__mean', 'Campground',\n",
    "       'FireStation_Dist', 'Airfield_Dist', 'City_Bounds', 'NPS_Bounds',\n",
    "       'Lightning', 'Elev', 'Housing_Density'], outPath = 'C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pixel_id.1</th>\n",
       "      <th>time</th>\n",
       "      <th>aet__maximum</th>\n",
       "      <th>aet__mean</th>\n",
       "      <th>cwd__maximum</th>\n",
       "      <th>cwd__mean</th>\n",
       "      <th>Campground</th>\n",
       "      <th>FireStation_Dist</th>\n",
       "      <th>...</th>\n",
       "      <th>Roughness</th>\n",
       "      <th>Topo_Heterogeneity</th>\n",
       "      <th>City_Bounds</th>\n",
       "      <th>NPS_Bounds</th>\n",
       "      <th>Lightning</th>\n",
       "      <th>Elev</th>\n",
       "      <th>Housing_Density</th>\n",
       "      <th>year</th>\n",
       "      <th>pixel_group</th>\n",
       "      <th>year_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134588</td>\n",
       "      <td>134588</td>\n",
       "      <td>134588</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>104.054550</td>\n",
       "      <td>38.798958</td>\n",
       "      <td>75.966362</td>\n",
       "      <td>40.992970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>36.043740</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073291</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1975</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134596</td>\n",
       "      <td>134596</td>\n",
       "      <td>134596</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>155.080505</td>\n",
       "      <td>58.283031</td>\n",
       "      <td>42.300999</td>\n",
       "      <td>20.542719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26248.809</td>\n",
       "      <td>...</td>\n",
       "      <td>41.402580</td>\n",
       "      <td>436.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073291</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1975</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>135555</td>\n",
       "      <td>135555</td>\n",
       "      <td>135555</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>104.839996</td>\n",
       "      <td>35.365177</td>\n",
       "      <td>89.362000</td>\n",
       "      <td>42.025677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35383.613</td>\n",
       "      <td>...</td>\n",
       "      <td>45.124300</td>\n",
       "      <td>506.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114690</td>\n",
       "      <td>636.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1975</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136483</td>\n",
       "      <td>136483</td>\n",
       "      <td>136483</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>164.182999</td>\n",
       "      <td>62.888699</td>\n",
       "      <td>34.011501</td>\n",
       "      <td>15.720217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26419.690</td>\n",
       "      <td>...</td>\n",
       "      <td>57.852970</td>\n",
       "      <td>634.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073291</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1975</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136523</td>\n",
       "      <td>136523</td>\n",
       "      <td>136523</td>\n",
       "      <td>197501_197912</td>\n",
       "      <td>144.088501</td>\n",
       "      <td>52.848095</td>\n",
       "      <td>54.305500</td>\n",
       "      <td>22.716408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57775.426</td>\n",
       "      <td>...</td>\n",
       "      <td>44.113197</td>\n",
       "      <td>1119.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182597</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_id  Unnamed: 0  pixel_id.1           time  aet__maximum  aet__mean  \\\n",
       "0    134588      134588      134588  197501_197912    104.054550  38.798958   \n",
       "1    134596      134596      134596  197501_197912    155.080505  58.283031   \n",
       "2    135555      135555      135555  197501_197912    104.839996  35.365177   \n",
       "3    136483      136483      136483  197501_197912    164.182999  62.888699   \n",
       "4    136523      136523      136523  197501_197912    144.088501  52.848095   \n",
       "\n",
       "   cwd__maximum  cwd__mean  Campground  FireStation_Dist     ...      \\\n",
       "0     75.966362  40.992970         0.0         25000.000     ...       \n",
       "1     42.300999  20.542719         0.0         26248.809     ...       \n",
       "2     89.362000  42.025677         0.0         35383.613     ...       \n",
       "3     34.011501  15.720217         0.0         26419.690     ...       \n",
       "4     54.305500  22.716408         0.0         57775.426     ...       \n",
       "\n",
       "   Roughness  Topo_Heterogeneity  City_Bounds  NPS_Bounds  Lightning    Elev  \\\n",
       "0  36.043740               274.0          0.0         0.0   0.073291    72.0   \n",
       "1  41.402580               436.0          0.0         0.0   0.073291   307.0   \n",
       "2  45.124300               506.0          0.0         0.0   0.114690   636.0   \n",
       "3  57.852970               634.0          0.0         0.0   0.073291   318.0   \n",
       "4  44.113197              1119.0          0.0         0.0   0.182597  1207.0   \n",
       "\n",
       "   Housing_Density  year  pixel_group  year_group  \n",
       "0             0.75  1975            9           0  \n",
       "1             0.00  1975            4           0  \n",
       "2             0.00  1975            5           0  \n",
       "3             0.00  1975            5           0  \n",
       "4             0.00  1975            6           0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combined_Data with groups\n",
    "CrossVal_Output[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_id</th>\n",
       "      <th>value</th>\n",
       "      <th>year</th>\n",
       "      <th>pixel_group</th>\n",
       "      <th>year_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>135555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_id  value  year  pixel_group  year_group\n",
       "0    134588    0.0  1975            9           0\n",
       "1    134596    0.0  1975            4           0\n",
       "2    135555    0.0  1975            5           0\n",
       "3    136483    0.0  1975            5           0\n",
       "4    136523    0.0  1975            6           0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#target Data with groups\n",
    "CrossVal_Output[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixels_Years_MSE</th>\n",
       "      <th>Pixels_MSE</th>\n",
       "      <th>Years_MSE</th>\n",
       "      <th>Pixels_Years_R2</th>\n",
       "      <th>Pixels_R2</th>\n",
       "      <th>Years_R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>-183.852539</td>\n",
       "      <td>-237.839879</td>\n",
       "      <td>-232.131489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>-304.769247</td>\n",
       "      <td>-192.016342</td>\n",
       "      <td>-324.321465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002605</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>-117.270600</td>\n",
       "      <td>-157.814309</td>\n",
       "      <td>-141.590831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008178</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.006358</td>\n",
       "      <td>-236.228321</td>\n",
       "      <td>-304.617249</td>\n",
       "      <td>-232.445290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>-406.673984</td>\n",
       "      <td>-298.379209</td>\n",
       "      <td>-359.692282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pixels_Years_MSE  Pixels_MSE  Years_MSE  Pixels_Years_R2   Pixels_R2  \\\n",
       "0          0.002549    0.005027   0.006741      -183.852539 -237.839879   \n",
       "1          0.005529    0.002535   0.003687      -304.769247 -192.016342   \n",
       "2         -0.002605    0.006618   0.000964      -117.270600 -157.814309   \n",
       "3          0.008178    0.004954   0.006358      -236.228321 -304.617249   \n",
       "4          0.002715    0.006497   0.003655      -406.673984 -298.379209   \n",
       "\n",
       "     Years_R2  \n",
       "0 -232.131489  \n",
       "1 -324.321465  \n",
       "2 -141.590831  \n",
       "3 -232.445290  \n",
       "4 -359.692282  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary MSE and R2 for all runs, against spatially novel, temporally novel, and completely novel data\n",
    "CrossVal_Output[2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.002548767869070101,\n",
       "  -183.852538559554),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.0055289764486881054,\n",
       "  -304.7692467859652),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  -0.0026053775755709996,\n",
       "  -117.27060044242097),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.008177840945204906,\n",
       "  -236.22832114126788),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.002714902882150816,\n",
       "  -406.6739839965974),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.004828924215600772,\n",
       "  -166.1432365041732),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.006976549528315167,\n",
       "  -197.44813187707422),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.0024701813217715696,\n",
       "  -284.1876796364954),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  -0.005748537793546226,\n",
       "  -98.68458231106764),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.005804668305897876,\n",
       "  -205.87509339226878),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.0035729067356526922,\n",
       "  -324.69134945956847),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.001551835901608345,\n",
       "  -146.42899102747066),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.004479646174767149,\n",
       "  -170.08473033772287),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.00374759309384709,\n",
       "  -329.37800414305735),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.0012315644765859002,\n",
       "  -114.93069478190559),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.006617219681100406,\n",
       "  -301.8683665375517),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.00173901173790314,\n",
       "  -373.3685382705611),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.0018835907384407993,\n",
       "  -164.7000508481198),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.00254823441281693,\n",
       "  -258.02055926237625),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.002177361022343116,\n",
       "  -402.4620788600129),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.001690859896151142,\n",
       "  -168.79404227529648),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.0077222063897584015,\n",
       "  -277.27384372947967),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.004065236145547657,\n",
       "  -370.70166851507616),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  -0.00043188634184665453,\n",
       "  -162.00917031437768),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.008544516958942205,\n",
       "  -242.43562882479185),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.0020823302785382136,\n",
       "  -347.6107025604139),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  -0.0003724329450940367,\n",
       "  -125.76875906405083),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.004861324488968699,\n",
       "  -184.16917801527725),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.007377581550899848,\n",
       "  -255.71296835362932),\n",
       " (ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  -0.0008945164339697609,\n",
       "  -134.95577402372422)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of models\n",
    "CrossVal_Output[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980],\n",
       " [2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980],\n",
       " [2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980],\n",
       " [2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980],\n",
       " [2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980],\n",
       " [2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980],\n",
       " [2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980],\n",
       " [2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980],\n",
       " [2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980],\n",
       " [2000, 1975],\n",
       " [1995, 2005, 1990],\n",
       " [1985, 2010, 1980]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of years excluded from training data for each model run\n",
    "CrossVal_Output[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1_ratio': 0.48499999999999976, 'alpha': 0.8}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parameters selected during hypertuning\n",
    "CrossVal_Output[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00419769 -0.0041972  -0.00419661 ... -0.00097092 -0.00100184\n",
      " -0.00103253]\n"
     ]
    }
   ],
   "source": [
    "'''Predict fire risk within period of interest using elastic net regularization- train model on combined_Data across all available years except within period of interest,\n",
    "    and save resulting predictions as csv and as tif to location 'outPath'\n",
    "    \n",
    "    :param combined_Data_Training: dataFrame including all desired explanatory factors \n",
    "            across all locations & years to be used in training model\n",
    "    :param target_Data_Training: dataFrame including observed fire occurrences \n",
    "            across all locations & years to be used in training model\n",
    "    :param preMasked_Data_Path: file path to location of files to use in predicting fire risk \n",
    "                    (note - these files should not have undergone Poisson disk masking)\n",
    "    :param outPath: desired output location for predicted fire risk files (csv, pickle, and tif)\n",
    "    :param year_List: list of years for which predictions are desired\n",
    "    :param Datafields: list of explanatory factors to be intered into model\n",
    "    :param mask: filepath of raster mask to be used in masking output predictions, \n",
    "            and as an example raster for choosing array shape and projections for .tif output files\n",
    "    :param params: parameters for elastic net regression (presumably developed from 2dimCrossval)\n",
    "    :return:  returns a list of all models, accompanied by a list of years being predicted \n",
    "            - note - return output is equivalent to data exported as models.pickle\n",
    "            '''\n",
    "\n",
    "q = model.elastic_YearPredictor(combined_Data, target_Data, \n",
    "                  preMasked_Data_Path = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Pre_Masked_5Len/\",\n",
    "                  outPath = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Test_Preds/\",  \n",
    "                  year_List = list(range(2015, 2020, 5)), \n",
    "                    periodLen = 5,\n",
    "                  DataFields = ['aet__mean', 'cwd__maximum', 'cwd__mean', 'Campground',\n",
    "       'FireStation_Dist', 'Airfield_Dist', 'City_Bounds', 'NPS_Bounds',\n",
    "       'Lightning', 'Elev', 'Housing_Density'],\n",
    "                 mask = r\"../Data/Examples/buffer/StatePoly_buf.tif\",\n",
    "                    params = CrossVal_Output[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixels_Years MSE Overall:  0.021930265361375604\n",
      "pixels_Years R2 Overall:  -41.56068062536632\n",
      "\n",
      "\n",
      "pixels MSE Overall:  0.031332130572446715\n",
      "pixels R2 Overall:  -38.36478623236376\n",
      "\n",
      "\n",
      "years MSE Overall:  0.023890239933704382\n",
      "years R2 Overall:  -39.755598205586914\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Conduct random Forest regressions on data, with k-fold cross-validation conducted independently \n",
    "      across both years and pixels. \n",
    "      Returns mean model MSE and R2 when predicting fire risk at \n",
    "      A) locations outside of the training dataset\n",
    "      B) years outside of the training dataset\n",
    "      C) locations and years outside of the training dataset\n",
    "\n",
    "    Returns a list of objects, consisting of:\n",
    "      0: Combined_Data file with testing/training groups labeled\n",
    "      1: Target Data file with testing/training groups labeled\n",
    "      2: summary dataFrame of MSE and R2 for each model run\n",
    "          (against holdout data representing either novel locations, novel years, or both)\n",
    "      3: list of random forest models for use in predicting Fires in further locations/years\n",
    "      4: list of list of years not used in model training for each run\n",
    "  '''\n",
    "\n",
    "  #param combined_Data: explanatory factors to be used in predicting fire risk\n",
    "  #param target_Data: observed fire occurrences\n",
    "  #param varsToGroupBy: list of (2) column names from combined_Data & target_Data to be used in creating randomized groups\n",
    "  #param groupVars: list of (2) desired column names for the resulting randomized groups\n",
    "  #param testGroups: number of distinct groups into which data sets should be divided (for each of two variables) \n",
    "  #param DataFields: column names of data to be used in modelling\n",
    "  #param outPath:  Location in which to export pickle files and summary statistics\n",
    "  #param params: list of parameters to be used in hypertuning\n",
    "\n",
    "\n",
    "q = CrossVal_Output = model.RandomForestReg_2dimTest(combined_Data, target_Data, [\"pixel_id\", \"year\"], [\"pixel_group\", \"year_group\"], \n",
    "                                            testGroups = [10, 3], \n",
    "                                            DataFields = ['aet__mean', 'cwd__maximum', 'cwd__mean', 'Campground',\n",
    "       'FireStation_Dist', 'Airfield_Dist', 'City_Bounds', 'NPS_Bounds',\n",
    "       'Lightning', 'Elev', 'Housing_Density'], outPath = 'C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00351779 0.00351779 0.00351779 ... 0.00340962 0.00336616 0.00336822]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
       "              max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0, min_impurity_split=None,\n",
       "              min_samples_leaf=10, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0, n_estimators=100, n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False)]],\n",
       " [2015])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Predict fire risk within period of interest using random forest regression- train model on combined_Data across all available years except year of interest\n",
    "    save resulting predictions as csv and as tif to location 'outPath'\n",
    "    \n",
    "    :param combined_Data_Training: dataFrame including all desired explanatory factors \n",
    "            across all locations & years to be used in training model\n",
    "    :param target_Data_Training: dataFrame including observed fire occurrences \n",
    "            across all locations & years to be used in training model\n",
    "    :param preMasked_Data_Path: file path to location of files to use in predicting fire risk \n",
    "                    (note - these files should not have undergone Poisson disk masking)\n",
    "    :param outPath: desired output location for predicted fire risk files (csv, pickle, and tif)\n",
    "    :param year_List: list of years for which predictions are desired\n",
    "    :param Datafields: list of explanatory factors to be intered into model\n",
    "    :param mask: filepath of raster mask to be used in masking output predictions, \n",
    "            and as an example raster for choosing array shape and projections for .tif output files\n",
    "    :param params: parameters for random forest regression (presumably developed from 2dimCrossval)\n",
    "    :return:  returns a list of all models, accompanied by a list of years being predicted \n",
    "            - note - return output is equivalent to data exported as models.pickle\n",
    "'''\n",
    "\n",
    "model.randomForestReg_YearPredictor(combined_Data, target_Data, \n",
    "                  preMasked_Data_Path = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Pre_Masked_5Len/\",\n",
    "                  outPath = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Test_Preds/\",  \n",
    "                  year_List = list(range(2015, 2020, 5)), \n",
    "                    periodLen = 5,\n",
    "                  DataFields = ['aet__mean', 'cwd__maximum', 'cwd__mean', 'Campground',\n",
    "       'FireStation_Dist', 'Airfield_Dist', 'City_Bounds', 'NPS_Bounds',\n",
    "       'Lightning', 'Elev', 'Housing_Density'],\n",
    "                 mask = r\"../Data/Examples/buffer/StatePoly_buf.tif\", params = q[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Python3\\Anaconda3\\envs\\ts-raster\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixels_Years MSE Overall:  0.019428827953334355\n",
      "pixels_Years R2 Overall:  -33.66389849862572\n",
      "\n",
      "\n",
      "pixels MSE Overall:  0.03360051302278306\n",
      "pixels R2 Overall:  -32.65973505873693\n",
      "\n",
      "\n",
      "years MSE Overall:  0.02275388147150962\n",
      "years R2 Overall:  -33.41626161264868\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " '''Conduct random XGBoost regressions on data, with k-fold cross-validation conducted independently \n",
    "      across both years and pixels. \n",
    "      Returns mean model MSE and R2 when predicting fire risk at \n",
    "      A) locations outside of the training dataset\n",
    "      B) years outside of the training dataset\n",
    "      C) locations and years outside of the training dataset\n",
    "\n",
    "    Returns a list of objects, consisting of:\n",
    "      0: Combined_Data file with testing/training groups labeled\n",
    "      1: Target Data file with testing/training groups labeled\n",
    "      2: summary dataFrame of MSE and R2 for each model run\n",
    "          (against holdout data representing either novel locations, novel years, or both)\n",
    "      3: list of random forest models for use in predicting Fires in further locations/years\n",
    "      4: list of list of years not used in model training for each run\n",
    "  '''\n",
    "\n",
    "  #param combined_Data: explanatory factors to be used in predicting fire risk\n",
    "  #param target_Data: observed fire occurrences\n",
    "  #param varsToGroupBy: list of (2) column names from combined_Data & target_Data to be used in creating randomized groups\n",
    "  #param groupVars: list of (2) desired column names for the resulting randomized groups\n",
    "  #param testGroups: number of distinct groups into which data sets should be divided (for each of two variables) \n",
    "  #param DataFields: column names of data to be used in modelling\n",
    "  #param outPath:  Location in which to export pickle files and summary statistics\n",
    "  #param params: list of parameters to be used in hypertuning\n",
    "\n",
    "q = CrossVal_Output = model.XGBoostReg_2dimTest(combined_Data, target_Data, [\"pixel_id\", \"year\"], [\"pixel_group\", \"year_group\"], \n",
    "                                            testGroups = [10, 3], \n",
    "                                            DataFields = ['aet__mean', 'cwd__maximum', 'cwd__mean', 'Campground',\n",
    "       'FireStation_Dist', 'Airfield_Dist', 'City_Bounds', 'NPS_Bounds',\n",
    "       'Lightning', 'Elev', 'Housing_Density'], outPath = 'C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01861781 -0.01861781 -0.01861781 ... -0.02599061 -0.02878857\n",
      " -0.02878857]\n"
     ]
    }
   ],
   "source": [
    "'''predict fire risk within period of interest using XGBoost regression- train model on combined_Data across all available years except year of interest\n",
    "    save resulting predictions as csv and as tif to location 'outPath'\n",
    "    \n",
    "    :param combined_Data_Training: dataFrame including all desired explanatory factors \n",
    "            across all locations & years to be used in training model\n",
    "    :param target_Data_Training: dataFrame including observed fire occurrences \n",
    "            across all locations & years to be used in training model\n",
    "    :param preMasked_Data_Path: file path to location of files to use in predicting fire risk \n",
    "                    (note - these files should not have undergone Poisson disk masking)\n",
    "    :param outPath: desired output location for predicted fire risk files (csv, pickle, and tif)\n",
    "    :param year_List: list of years for which predictions are desired\n",
    "    :param Datafields: list of explanatory factors to be intered into model\n",
    "    :param mask: filepath of raster mask to be used in masking output predictions, \n",
    "            and as an example raster for choosing array shape and projections for .tif output files\n",
    "    :param params: parameters for random forest regression (presumably developed from 2dimCrossval)\n",
    "    :return:  returns a list of all models, accompanied by a list of years being predicted \n",
    "            - note - return output is equivalent to data exported as models.pickle\n",
    "    '''\n",
    "\n",
    "\n",
    "A = model.XGBoostReg_YearPredictor(combined_Data, target_Data, \n",
    "                  preMasked_Data_Path = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Pre_Masked_5Len/\",\n",
    "                  outPath = \"C:/Users/Python3/Documents/wildfire_FRAP_working/wildfire_FRAP/Data/Extracted_Features/Test_Preds/\",  \n",
    "                  year_List = list(range(2015, 2020, 5)), \n",
    "                    periodLen = 5,\n",
    "                  DataFields = ['aet__mean', 'cwd__maximum', 'cwd__mean', 'Campground',\n",
    "       'FireStation_Dist', 'Airfield_Dist', 'City_Bounds', 'NPS_Bounds',\n",
    "       'Lightning', 'Elev', 'Housing_Density'],\n",
    "                 mask = r\"../Data/Examples/buffer/StatePoly_buf.tif\", params = q[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
